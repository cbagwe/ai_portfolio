{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d14963",
   "metadata": {},
   "source": [
    "# Automatic identification and structuring of AI companies for product creation\n",
    "\n",
    "### Author: Chaitali Suhas Bagwe (cbagwe@mail.uni-paderborn.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98cecd7",
   "metadata": {},
   "source": [
    "***\n",
    "#### Install Necessary Libraries\n",
    "\n",
    "Remove the comment char '#' for installing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa37aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4fda6",
   "metadata": {},
   "source": [
    "***\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04adb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm,metrics\n",
    "from urllib.request import Request, urlopen\n",
    "import yake\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25903492",
   "metadata": {},
   "source": [
    "***\n",
    "#### Read the data from the Excel File\n",
    "In this block we read the data from excel file to the pandas dataframe and change the column name of \"Relevance of service/app and fit to the portfolio\" to \"Relevance\" for better readability/extraction. Since we are only interested in product engineering companies we will mark the \"1 - product engineering\" as \"1\" and others as \"0\". This makes the classification problem into binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ce50dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chaitali bagwe\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "433    0\n",
       "434    1\n",
       "435    1\n",
       "436    0\n",
       "437    0\n",
       "Name: Relevance, Length: 438, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the input Excel file\n",
    "dataset = pd.read_excel('App and service store_Long List.xlsx', sheet_name='App and service store_Long List')\n",
    "\n",
    "#Change column names\n",
    "dataset.rename(columns = {'Relevance of service/app and fit to the portfolio':'Relevance'}, inplace = True)\n",
    "\n",
    "#Change Relevance values\n",
    "dataset.Relevance.replace('1 - product engineering',1, inplace = True)\n",
    "dataset.Relevance.replace('2 - production',0, inplace = True)\n",
    "dataset.Relevance.replace('3- AI general',0, inplace = True)\n",
    "dataset.Relevance.replace('4 - cross sectional processes',0, inplace = True)\n",
    "dataset.Relevance.replace('5 - not relevant',0, inplace = True)\n",
    "\n",
    "dataset[\"Relevance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ef197",
   "metadata": {},
   "source": [
    "***\n",
    "#### Segregration of Test and Train data for our ML model\n",
    "The sampling helps us in having random and not fixed input to the model. We have used 60 samples of product engineering companies and 60 samples of other companies as our training dataset. Thus the size of training data is 120. For test data we have used random 20 samples from each type of companies. Thus the size of testing data is 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05abbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregrate the product engineering and other data from the dataset\n",
    "prod_engg_data = dataset[dataset['Relevance'] == 1]\n",
    "other_data = dataset[dataset['Relevance'] == 0]\n",
    "\n",
    "# Generate train and test data by random sampling of product engineering and other data and then appending it together.\n",
    "train_data = prod_engg_data.sample(60).append(other_data.sample(60))\n",
    "test_data = prod_engg_data.sample(20).append(other_data.sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddaa311",
   "metadata": {},
   "source": [
    "***\n",
    "### Text Preprocessing\n",
    "\n",
    "#### 1) Setting up stop-words and stemmer\n",
    "Stop Words : Removing commonly used english stop words like *and*, *the*, *a*, *an*, etc. We have also removed the words that are not useful for us in the context of websites. These include the words from header and footer of the websites, dropdown menu words of the websites, terms and conditions, etc.\n",
    "\n",
    "Stemmer : Reducing the word to its word stem; *extraction* -> *extract*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf1e9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# Add custom stop words (frequently occuring but add no value)\n",
    "stop_words += ['about', 'us', 'contact', 'how','login', 'hello','email','home','blog','terms','conditions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967916f",
   "metadata": {},
   "source": [
    "***\n",
    "#### 2) Clean Text Function\n",
    "\n",
    "This function cleans the text received by the variable \"text\". Cleaning includes removal of stopwords, white spaces, html tags, numbers, special characters and punctuations. This function also tokenizes the text sent to it and performs stemming on each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea24ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    # remove white spaces, html tags, numbers, special characters, punctuations\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    # perform stemming on each word\n",
    "    words_filtered = [\n",
    "        stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "    ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4dcc32",
   "metadata": {},
   "source": [
    "***\n",
    "#### 3) Initialize the keyword extractor from Yake library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14727ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "num_of_keywords = 30\n",
    "window_size = 1\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(n=max_ngram_size, dedupLim=deduplication_threshold, top=num_of_keywords, windowsSize=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b65c8",
   "metadata": {},
   "source": [
    "***\n",
    "### Miscellaneous Functions\n",
    "\n",
    "#### 1) Read Content from URL of the company\n",
    "\n",
    "BeautifulSoup library is used for the extraction of content from the given URL. Additionally, the footer attributes and the website's cookies attributes are removed in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ab2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_url_content(URL,page):\n",
    "    # read the content\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # remove the website cookies content\n",
    "    for div in soup.find_all('div', attrs={'data-nosnippet' : 'true'}):\n",
    "        div.decompose()\n",
    "        \n",
    "    # remove footer\n",
    "    for footer in soup.find_all('footer'):\n",
    "        footer.decompose()\n",
    "        \n",
    "    # return the cleaned content\n",
    "    return clean_text(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcee12d",
   "metadata": {},
   "source": [
    "***\n",
    "#### 2) Parse the required contents from the dataframe.\n",
    "This function reads the data from the dataframe and passes the URL of each company to the read_url_content() function. A try and catch block is added for checking if the URL is accessible or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2088a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_webdata(dataframe):\n",
    "    # create an empty list of page data\n",
    "    page_data = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        URL = row[\"Link\"]\n",
    "        try:\n",
    "            # access the URL\n",
    "            page = requests.get(URL)\n",
    "            # append the URL content to the list\n",
    "            page_data.append(read_url_content(URL,page))\n",
    "        except(ConnectionError, Exception):\n",
    "            # for websites not accessible append empty string to the list\n",
    "            page_data.append(\"\")\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefe9c3",
   "metadata": {},
   "source": [
    "***\n",
    "#### 3) Metrics Function\n",
    "Calculates the metrics for the given actual and predicted datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb270557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(actual, predicted):\n",
    "    avg = 'weighted'\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(actual, predicted))\n",
    "    print(\"Precision:\",metrics.precision_score(actual, predicted, average=avg))\n",
    "    print(\"Recall:\",metrics.recall_score(actual, predicted, average=avg))\n",
    "    print(\"F1 score:\",metrics.f1_score(actual, predicted, average=avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026589a",
   "metadata": {},
   "source": [
    "***\n",
    "#### 4) Extract Keywords Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a3ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(string):\n",
    "    keywords = kw_extractor.extract_keywords(string)\n",
    "    keywords = [x for (x,_) in keywords]\n",
    "    return \" \".join(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227d32d",
   "metadata": {},
   "source": [
    "***\n",
    "#### Get the cleaned webdata for train and test dataset\n",
    "The cleaned webdata is stored in the \"WebData\" column created in pandas dataframe for train and test dataset respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33a00b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Clean train and test data   \n",
    "train_data[\"WebData\"] = get_cleaned_webdata(train_data)\n",
    "test_data[\"WebData\"] = get_cleaned_webdata(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a770de",
   "metadata": {},
   "source": [
    "***\n",
    "#### Perform keyword extraction on the cleaned webdata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0856137",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake = \"true\"\n",
    "if yake == \"true\":\n",
    "    train_data[\"Keywords\"] = train_data[\"WebData\"].apply(extract_keywords)\n",
    "    test_data[\"Keywords\"] = test_data[\"WebData\"].apply(extract_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af5fa77",
   "metadata": {},
   "source": [
    "***\n",
    "#### Feature extraction using TF-IDF\n",
    "TF-IDF measures how relvant a word is when compared with the entire document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6987d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizor = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9658a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if yake == \"true\":\n",
    "    # Transform the train and test cleaned input webdata to tfidf vectors respectively\n",
    "    train_input = tfidf_vectorizor.fit_transform(train_data[\"Keywords\"]).toarray().tolist()\n",
    "    test_input = tfidf_vectorizor.transform(test_data[\"Keywords\"]).toarray().tolist()\n",
    "else:\n",
    "    # Transform the train and test cleaned input webdata to tfidf vectors respectively\n",
    "    train_input = tfidf_vectorizor.fit_transform(train_data[\"WebData\"]).toarray().tolist()\n",
    "    test_input = tfidf_vectorizor.transform(test_data['WebData']).toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54c722d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train and test output data to list.\n",
    "train_output = train_data['Relevance'].tolist()\n",
    "test_output = test_data['Relevance'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f48f2",
   "metadata": {},
   "source": [
    "***\n",
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb64fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(train_input, train_output)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59aebaa",
   "metadata": {},
   "source": [
    "***\n",
    "#### Print the statistics for SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d73ad859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n",
      "Precision: 0.9040404040404042\n",
      "Recall: 0.9\n",
      "F1 score: 0.8997493734335841\n"
     ]
    }
   ],
   "source": [
    "print_statistics(test_output,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a8bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df1e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115cf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93b9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498438fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup_initial = re.sub(\"\\s{2,}[a-z]+\\s{2,}\", \" \", soup.text,flags=re.IGNORECASE)\n",
    "#print(clean_text(soup.text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
